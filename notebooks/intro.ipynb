{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6617ea22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode: cluster\n",
      "Reading sample from: /data/data/sample.csv\n",
      "Writing outputs to: /shared\n",
      "Using cluster mode with master: spark://spark-master:7077\n",
      "Setting spark.driver.host -> jupyter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/10/16 11:34:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark master: spark://spark-master:7077\n",
      "Spark version: 4.0.0\n",
      "spark.driver.host: jupyter\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg, col\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os, socket, traceback\n",
    "\n",
    "# Detect cluster mode via env set in docker-compose\n",
    "spark_master = os.environ.get('SPARK_MASTER')\n",
    "\n",
    "# Resolve paths: in containers, repo is mounted at /data (RO), so CSVs live under /data/data\n",
    "cwd = os.getcwd()\n",
    "project_root = os.path.abspath(os.path.join(cwd, '..'))\n",
    "local_data = os.path.join(project_root, 'data')\n",
    "cluster_data = '/data/data'\n",
    "cluster_shared = '/shared'\n",
    "\n",
    "if spark_master:\n",
    "    data_dir = cluster_data\n",
    "    output_dir = cluster_shared\n",
    "else:\n",
    "    data_dir = local_data\n",
    "    output_dir = os.path.join(project_root, 'output')\n",
    "\n",
    "sample_csv = os.path.join(data_dir, 'sample.csv')\n",
    "print('Mode:', 'cluster' if spark_master else 'local')\n",
    "print('Reading sample from:', sample_csv)\n",
    "print('Writing outputs to:', output_dir)\n",
    "\n",
    "# Stop previous SparkSession if any\n",
    "try:\n",
    "    spark.stop()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Prefer explicit env; in docker-compose, workers can resolve 'jupyter'\n",
    "driver_host = os.environ.get('SPARK_DRIVER_HOST') or ('jupyter' if spark_master else socket.gethostname())\n",
    "\n",
    "# Build SparkSession with safe defaults (UI disabled to avoid MetricsSystem issues)\n",
    "builder = (\n",
    "    SparkSession.builder\n",
    "    .appName('notebook')\n",
    "    .config('spark.ui.enabled', 'true')\n",
    ")\n",
    "\n",
    "if spark_master:\n",
    "    print('Using cluster mode with master:', spark_master)\n",
    "    print('Setting spark.driver.host ->', driver_host)\n",
    "    builder = (\n",
    "        builder\n",
    "        .master(spark_master)\n",
    "        .config('spark.driver.host', driver_host)\n",
    "        .config('spark.driver.bindAddress', '0.0.0.0')\n",
    "    )\n",
    "else:\n",
    "    print('No SPARK_MASTER -> using local[*]')\n",
    "    builder = builder.master('local[*]')\n",
    "\n",
    "try:\n",
    "    spark = builder.getOrCreate()\n",
    "except Exception as e:\n",
    "    print('SparkSession.getOrCreate() failed:', e)\n",
    "    traceback.print_exc()\n",
    "    print('Retrying with event log disabled')\n",
    "    builder = builder.config('spark.eventLog.enabled', 'false')\n",
    "    spark = builder.getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "print('Spark master:', sc.master)\n",
    "print('Spark version:', spark.version)\n",
    "print('spark.driver.host:', spark.conf.get('spark.driver.host'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7fbe6d",
   "metadata": {},
   "source": [
    "Look at you SPARK UI !!\n",
    "Maybe -> localhost:4040"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d814ee2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 4) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15]]\n",
      "Partition 0: [0, 1, 2, 3]\n",
      "Partition 1: [4, 5, 6, 7]\n",
      "Partition 2: [8, 9, 10, 11]\n",
      "Partition 3: [12, 13, 14, 15]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "rdd=sc.parallelize(range(16),4)\n",
    "print(rdd.getNumPartitions())\n",
    "partitioned_data=rdd.glom().collect()\n",
    "print(partitioned_data)\n",
    "for i, part in enumerate(partitioned_data):\n",
    "    print(f\"Partition {i}: {part}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ada91503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered RDD (even numbers): [0, 2, 4, 6, 8, 10, 12, 14]\n",
      "Original RDD still intact: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "lineage (to debug): (4) PythonRDD[2] at collect at /tmp/ipykernel_222/1110730522.py:2 []\n",
      " |  ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:297 []\n"
     ]
    }
   ],
   "source": [
    "rdd2_filtered = rdd.filter(lambda x: x % 2 == 0)\n",
    "print('Filtered RDD (even numbers):', rdd2_filtered.collect())\n",
    "print('Original RDD still intact:', rdd.collect())\n",
    "print('lineage (to debug):', rdd2_filtered.toDebugString().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67533b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapped RDD sample (x, x*2): [(0, 0), (2, 4), (4, 8), (6, 12), (8, 16), (10, 20), (12, 24), (14, 28)]\n",
      "lineage (to debug): (4) pairs_even PythonRDD[4] at RDD at PythonRDD.scala:56 []\n",
      " |  ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:297 []\n"
     ]
    }
   ],
   "source": [
    "rdd3_mapped = rdd2_filtered.map(lambda x: (x, x * 2)).setName('pairs_even')\n",
    "print('Mapped RDD sample (x, x*2):', rdd3_mapped.take(10))\n",
    "# Lineage du RDD mappé\n",
    "dbg = rdd3_mapped.toDebugString()\n",
    "print('lineage (to debug):', dbg.decode('utf-8') if hasattr(dbg, 'decode') else dbg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2d84cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0), (2, 4), (4, 8), (6, 12), (8, 16)]\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "## is rdd3_mapped recomputed ??\n",
    "print(rdd3_mapped.take(5))\n",
    "print(rdd3_mapped.count())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005e8b3a",
   "metadata": {},
   "source": [
    "https://www.gutenberg.org/cache/epub/2701/pg2701.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "306a9901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 1246k  100 1246k    0     0  1402k      0 --:--:-- --:--:-- --:--:-- 1402k\n",
      "100 1246k  100 1246k    0     0  1402k      0 --:--:-- --:--:-- --:--:-- 1402k\n"
     ]
    }
   ],
   "source": [
    "# Téléchargement de Moby Dick (Project Gutenberg) vers data_dir\n",
    "# -L pour suivre les redirections, -o pour choisir le chemin de sortie\n",
    "!mkdir -p {data_dir} && curl -L -o {data_dir}/mobydick.txt https://www.gutenberg.org/cache/epub/2701/pg2701.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aef320d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:=============================>                            (1 + 1) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 mots dans Moby Dick:\n",
      "the: 14727\n",
      "of: 6746\n",
      "and: 6514\n",
      "a: 4805\n",
      "to: 4709\n",
      "in: 4244\n",
      "that: 3100\n",
      "it: 2537\n",
      "his: 2532\n",
      "i: 2127\n",
      "he: 1900\n",
      "s: 1827\n",
      "but: 1822\n",
      "with: 1770\n",
      "as: 1753\n",
      "is: 1748\n",
      "was: 1647\n",
      "for: 1644\n",
      "all: 1544\n",
      "this: 1441\n",
      "Résultats sauvegardés dans: /shared/mobydick_wordcount.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "import re\n",
    "moby_path = os.path.join(data_dir, 'mobydick.txt')\n",
    "\n",
    "# RDD et tokenization (pattern compilé hors des lambdas)\n",
    "pattern = re.compile(r\"[a-z']+\")\n",
    "\n",
    "def tokenize(line):\n",
    "    return pattern.findall(line.lower())\n",
    "\n",
    "text_rdd = sc.textFile(moby_path)\n",
    "words = text_rdd.flatMap(tokenize).filter(lambda w: len(w) > 0)\n",
    "counts = words.map(lambda w: (w, 1)).reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# Top 50 mots (par fréquence)\n",
    "top50 = counts.takeOrdered(50, key=lambda x: -x[1])\n",
    "\n",
    "# Affichage succinct (top 20)\n",
    "print(\"Top 20 mots dans Moby Dick:\")\n",
    "for w, c in top50[:20]:\n",
    "    print(f\"{w}: {c}\")\n",
    "\n",
    "# Sauvegarde CSV dans output_dir (création si besoin)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "out_csv = os.path.join(output_dir, 'mobydick_wordcount.csv')\n",
    "\n",
    "# Utilisation de pandas (importé dans le notebook) pour écrire un CSV lisible\n",
    "wc_df = pd.DataFrame(top50, columns=['word', 'count'])\n",
    "wc_df.to_csv(out_csv, index=False)\n",
    "print(\"Résultats sauvegardés dans:\", out_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81f26ea",
   "metadata": {},
   "source": [
    "How many Shuffle Read and Shuffle Write ??\n",
    "Look in Spark UI, is it normal ??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a5a402",
   "metadata": {},
   "source": [
    "### Pourquoi on ne voit pas `filter` dans la lineage ?\n",
    "\n",
    "Avec l’API RDD en PySpark, les transformations Python (filter, map, flatMap, …) sont \"enveloppées\" dans des nœuds génériques `PythonRDD` côté JVM. La lineage montre donc des `PythonRDD` et leurs parents (ex: `ParallelCollectionRDD`) mais ne liste pas les noms de fonctions Python comme `filter`. \n",
    "\n",
    "Pour un plan plus explicite des opérateurs, utilisez l’API DataFrame (ex: `df.filter(...).explain('formatted')`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8e00266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plan DataFrame avec Filter:\n",
      "== Physical Plan ==\n",
      "* Filter (2)\n",
      "+- * Scan ExistingRDD (1)\n",
      "\n",
      "\n",
      "(1) Scan ExistingRDD [codegen id : 1]\n",
      "Output [1]: [value#0L]\n",
      "Arguments: [value#0L], MapPartitionsRDD[18] at applySchemaToPythonRDD at NativeMethodAccessorImpl.java:0, ExistingRDD, UnknownPartitioning(0)\n",
      "\n",
      "(2) Filter [codegen id : 1]\n",
      "Input [1]: [value#0L]\n",
      "Condition : (isnotnull(value#0L) AND ((value#0L % 2) = 0))\n",
      "\n",
      "\n",
      "\n",
      "Aperçu:\n",
      "== Physical Plan ==\n",
      "* Filter (2)\n",
      "+- * Scan ExistingRDD (1)\n",
      "\n",
      "\n",
      "(1) Scan ExistingRDD [codegen id : 1]\n",
      "Output [1]: [value#0L]\n",
      "Arguments: [value#0L], MapPartitionsRDD[18] at applySchemaToPythonRDD at NativeMethodAccessorImpl.java:0, ExistingRDD, UnknownPartitioning(0)\n",
      "\n",
      "(2) Filter [codegen id : 1]\n",
      "Input [1]: [value#0L]\n",
      "Condition : (isnotnull(value#0L) AND ((value#0L % 2) = 0))\n",
      "\n",
      "\n",
      "\n",
      "Aperçu:\n",
      "-RECORD 0----\n",
      " value | 0   \n",
      "-RECORD 1----\n",
      " value | 2   \n",
      "-RECORD 2----\n",
      " value | 4   \n",
      "-RECORD 3----\n",
      " value | 6   \n",
      "-RECORD 4----\n",
      " value | 8   \n",
      "-RECORD 5----\n",
      " value | 10  \n",
      "-RECORD 6----\n",
      " value | 12  \n",
      "-RECORD 7----\n",
      " value | 14  \n",
      "\n",
      "-RECORD 0----\n",
      " value | 0   \n",
      "-RECORD 1----\n",
      " value | 2   \n",
      "-RECORD 2----\n",
      " value | 4   \n",
      "-RECORD 3----\n",
      " value | 6   \n",
      "-RECORD 4----\n",
      " value | 8   \n",
      "-RECORD 5----\n",
      " value | 10  \n",
      "-RECORD 6----\n",
      " value | 12  \n",
      "-RECORD 7----\n",
      " value | 14  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Démo DataFrame pour voir explicitement un Filter\n",
    "from pyspark.sql import Row\n",
    "\n",
    "df = spark.createDataFrame([Row(value=v) for v in range(16)])\n",
    "filtered_df = df.filter((col('value') % 2) == 0)\n",
    "print('Plan DataFrame avec Filter:')\n",
    "filtered_df.explain('formatted')\n",
    "print('\\nAperçu:')\n",
    "filtered_df.show(10, truncate=False, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4362d348",
   "metadata": {},
   "source": [
    "### Comment observer la recomputation\n",
    "\n",
    "Même sans lineage détaillée, tu peux voir la recomputation:\n",
    "- Jobs UI: chaque action (take, count, collect) déclenche un nouveau job. Tu verras plusieurs jobs successifs.\n",
    "- Accumulateurs: instrumenter le filter/map pour incrémenter des compteurs, et comparer entre deux actions.\n",
    "- Job groups: nommer les jobs pour les repérer facilement.\n",
    "- Après persist/cache: refaire une action ne réincrémente pas les accumulateurs (pas de recomputation).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14a65af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Démo: observer la recomputation avec accumulateurs\n",
    "passed_filter = sc.longAccumulator('passed_filter')\n",
    "passed_map = sc.longAccumulator('passed_map')\n",
    "\n",
    "def filt(x):\n",
    "    if x % 2 == 0:\n",
    "        passed_filter.add(1)\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def mapper(x):\n",
    "    passed_map.add(1)\n",
    "    return (x, x*2)\n",
    "\n",
    "base = sc.parallelize(range(1000), 4)\n",
    "pipe = base.filter(filt).map(mapper)\n",
    "\n",
    "sc.setJobGroup('recompute-1', 'Action take(5)')\n",
    "print('take(5):', pipe.take(5))\n",
    "print('accumulators after take -> filter:', passed_filter.value, 'map:', passed_map.value)\n",
    "\n",
    "sc.setJobGroup('recompute-2', 'Action count()')\n",
    "print('count():', pipe.count())\n",
    "print('accumulators after count -> filter:', passed_filter.value, 'map:', passed_map.value)\n",
    "\n",
    "# Maintenant on persiste pour éviter la recomputation\n",
    "pipe_p = pipe.persist()\n",
    "\n",
    "sc.setJobGroup('recompute-3', 'Action count() on cached')\n",
    "print('count cached:', pipe_p.count())\n",
    "print('accumulators after cached count -> filter:', passed_filter.value, 'map:', passed_map.value)\n",
    "\n",
    "pipe_p.unpersist()\n",
    "sc.clearJobGroup()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
